{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2db13d-7857-4908-8337-8c4add7dcd63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Author_ = \"Sevda Molani\"\n",
    "\n",
    "_copyright_ = \"2022 Sevda Molani\"\n",
    "\n",
    "_License_ = \"Institute for Systems Biology\"\n",
    "\n",
    "_Version_ = \"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ada9d0-2cdd-4b11-9ccc-7b977c535e17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3637546f-9607-48ea-bc45-e1552173a7c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"\"\"SELECT \n",
    "CONCAT(instance, pat_id) as patient_id,\n",
    "pat_id,\n",
    "instance,\n",
    "pat_enc_csn_id\n",
    "FROM \n",
    "rdp_phi_sandbox.sm_positive_delta_results_feb\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "268bde18-15b7-4d2c-ac0c-ba82778d017a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to create temporary views for max WHO scores\n",
    "def create_max_who_view(interval):\n",
    "    query = f\"\"\"\n",
    "    SELECT pat_id, instance, pat_enc_csn_id, admissiondatetime, onehourbase, max_who_{interval}\n",
    "    FROM (\n",
    "        SELECT sm_positive_delta_results_feb.pat_id, sm_positive_delta_results_feb.instance,\n",
    "               sm_positive_delta_results_feb.pat_enc_csn_id, admissiondatetime, onehourbase,\n",
    "               MAX(rdp_phi_sandbox.sm_who_scores_feb.who_score) as max_who_{interval}\n",
    "        FROM rdp_phi_sandbox.sm_who_scores_feb\n",
    "        INNER JOIN rdp_phi_sandbox.sm_positive_delta_results_feb \n",
    "            ON rdp_phi_sandbox.sm_who_scores_feb.patient_id = sm_positive_delta_results_feb.patient_id\n",
    "        WHERE TIMESTAMP(rdp_phi_sandbox.sm_who_scores_feb.record_dt) <= onehourbase + INTERVAL '{interval}' day\n",
    "          AND TIMESTAMP(rdp_phi_sandbox.sm_who_scores_feb.record_dt) >= onehourbase\n",
    "        GROUP BY sm_positive_delta_results_feb.pat_id, sm_positive_delta_results_feb.instance, \n",
    "                 sm_positive_delta_results_feb.pat_enc_csn_id, admissiondatetime, onehourbase\n",
    "    )\n",
    "    \"\"\"\n",
    "    tmp_view = spark.sql(query).dropDuplicates()\n",
    "    tmp_view.createOrReplaceTempView(f'tmp_{interval}')\n",
    "\n",
    "# Create views for intervals 1, 7, 14, 28, and 56 days\n",
    "for interval in [1, 7, 14, 28, 56]:\n",
    "    create_max_who_view(interval)\n",
    "\n",
    "# Joining the temporary views\n",
    "join_expr = ['pat_id', 'pat_enc_csn_id', 'instance', 'admissiondatetime', 'onehourbase']\n",
    "d4 = spark.table('tmp_56')\n",
    "for interval in [28, 14, 7, 1]:\n",
    "    d4 = d4.join(spark.table(f'tmp_{interval}'), join_expr, \"inner\")\n",
    "\n",
    "# Add patient_id column and drop duplicates\n",
    "d4 = d4.withColumn('patient_id', concat('instance', 'pat_id')).dropDuplicates()\n",
    "d4.createOrReplaceTempView('d4')\n",
    "\n",
    "# Create the first WHO date view\n",
    "tmp2 = spark.sql(\"\"\"\n",
    "    SELECT patient_id, pat_id, instance, pat_enc_csn_id, admissiondatetime, onehourbase, \n",
    "           max_who_1, max_who_7, max_who_14, max_who_28, max_who_56, first_who_date\n",
    "    FROM (\n",
    "        SELECT d4.patient_id, d4.pat_id, d4.pat_enc_csn_id, d4.instance, admissiondatetime, onehourbase,\n",
    "               max_who_1, max_who_7, max_who_14, max_who_28, max_who_56,\n",
    "               MIN(TIMESTAMP(rdp_phi_sandbox.sm_who_scores_feb.record_dt)) as first_who_date\n",
    "        FROM rdp_phi_sandbox.sm_who_scores_feb\n",
    "        INNER JOIN d4 ON rdp_phi_sandbox.sm_who_scores_feb.patient_id = d4.patient_id\n",
    "        WHERE TIMESTAMP(rdp_phi_sandbox.sm_who_scores_feb.record_dt) >= onehourbase - INTERVAL '24' hour\n",
    "          AND TIMESTAMP(rdp_phi_sandbox.sm_who_scores_feb.record_dt) <= onehourbase + INTERVAL '1' hour\n",
    "          AND rdp_phi_sandbox.sm_who_scores_feb.who_score IS NOT NULL\n",
    "        GROUP BY d4.patient_id, d4.pat_id, d4.pat_enc_csn_id, d4.instance, admissiondatetime, onehourbase, \n",
    "                 max_who_1, max_who_7, max_who_14, max_who_28, max_who_56\n",
    "    )\n",
    "\"\"\").dropDuplicates()\n",
    "tmp2.createOrReplaceTempView('tmp2')\n",
    "\n",
    "# Create the first WHO view\n",
    "tmp3 = spark.sql(\"\"\"\n",
    "    SELECT pat_id, instance, pat_enc_csn_id, admissiondatetime, onehourbase, \n",
    "           max_who_1, max_who_7, max_who_14, max_who_28, max_who_56, first_who_date, first_who\n",
    "    FROM (\n",
    "        SELECT tmp2.pat_id, tmp2.instance, tmp2.pat_enc_csn_id, admissiondatetime, onehourbase,\n",
    "               max_who_1, max_who_7, max_who_14, max_who_28, max_who_56, first_who_date,\n",
    "               MIN(rdp_phi_sandbox.sm_who_scores_feb.who_score) as first_who\n",
    "        FROM rdp_phi_sandbox.sm_who_scores_feb\n",
    "        INNER JOIN tmp2 ON rdp_phi_sandbox.sm_who_scores_feb.patient_id = tmp2.patient_id\n",
    "        WHERE TIMESTAMP(rdp_phi_sandbox.sm_who_scores_feb.record_dt) = first_who_date\n",
    "        GROUP BY tmp2.pat_id, tmp2.instance, tmp2.pat_enc_csn_id, admissiondatetime, onehourbase,\n",
    "                 max_who_1, max_who_7, max_who_14, max_who_28, max_who_56, first_who_date\n",
    "    )\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "# Join the final result with df1\n",
    "df = df1.join(tmp3, ['pat_id', 'pat_enc_csn_id', 'instance'], 'left').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc99307b-9208-4922-9b82-6980b2671b9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_all_descendant_snomed_codes(code):\n",
    "  descendant_snomed_codes_df = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT\n",
    "    oc1.concept_code as ancestor_snomed_code,\n",
    "    oc1.concept_name as ancestor_concept_name,\n",
    "    oc2.concept_code as descendant_snomed_code,\n",
    "    oc2.concept_name as descendant_concept_name\n",
    "  FROM (\n",
    "    SELECT * FROM rdp_phi_sandbox.omop_concept_2020_08_05\n",
    "    WHERE\n",
    "      concept_code = {snomed_code} AND\n",
    "      vocabulary_id = 'SNOMED') as oc1\n",
    "  JOIN rdp_phi_sandbox.omop_concept_ancestor_2020_08_05 as oca\n",
    "  ON oc1.concept_id = oca.ancestor_concept_id\n",
    "  JOIN rdp_phi_sandbox.omop_concept_2020_08_05 as oc2\n",
    "  ON oca.descendant_concept_id = oc2.concept_id\n",
    "  ORDER BY min_levels_of_separation, oc2.concept_name\n",
    "  \"\"\".format(snomed_code=code))\n",
    "  \n",
    "  return descendant_snomed_codes_df\n",
    "\n",
    "\n",
    "def get_dsi_from_snomed_descendant(code):\n",
    "  query = \"SELECT * FROM rdp_phi_sandbox.jl_diagnosis_snomed_icd\" \n",
    "  df_dsi = spark.sql(query)\n",
    "  \n",
    "  df_snomed = get_all_descendant_snomed_codes(code)\n",
    "  df_snomed = df_snomed.select('descendant_snomed_code')\n",
    "  \n",
    "  df = df_dsi.join(df_snomed, df_dsi.SNOMED == df_snomed.descendant_snomed_code, \n",
    "                   how='inner').drop(df_snomed.descendant_snomed_code).drop(df_dsi.name)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc594cca-200c-4cea-bc48-899be79a7ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Reviewed codes - these are reviewed codes, including the additional GI factors\n",
    "\n",
    "rf2snomed = {} # dictionary for mapping the risk factor names to their snomed codes\n",
    "\n",
    "rf2snomed['htn'] = ['31992008', #Secondary\n",
    "                    '59621000', #Essential\n",
    "                   ] \n",
    "\n",
    "rf2snomed['asthma'] = ['195967001'] \n",
    "\n",
    "rf2snomed['COPD'] = [\n",
    "  '13645005', # Chronic obstructive lung disease (disorder)\n",
    "  '87433001', # Emphysema (disorder)\n",
    "  '185086009', # Chronic bronchitis\n",
    "]\n",
    "\n",
    "rf2snomed['stroke'] = ['230690007']\n",
    "\n",
    "rf2snomed['liver_disease'] = ['235856003']\n",
    "\n",
    "rf2snomed['ckd'] = ['709044004'] \n",
    "\n",
    "rf2snomed['hld'] = ['55822004','370992007'] #hyperlipidemia and dyslipidemia\n",
    "\n",
    "rf2snomed['osa'] = ['78275009'] \n",
    "\n",
    "rf2snomed['diabetes'] = ['44054006', #type 2\n",
    "                         '46635009' #type 1 \n",
    "                        ]\n",
    "\n",
    "rf2snomed['sot'] = ['739024006', #heart\n",
    "                    '739025007', #cild of hear and is heart-lung\n",
    "                    '737295003', #kidney\n",
    "                    '737297006', #liver\n",
    "                    '737296002' #lung\n",
    "                   ] # Solid organ transplantation\n",
    "\n",
    "rf2snomed['ics'] = ['737300001', #bone marrow\n",
    "                    '370388006', #immunocompromised\n",
    "                    '370391006', #immunosuppressed\n",
    "                    '234532001', #Immunodeficiency disorder\n",
    "                    '86406008', #HIV\n",
    "                    '62479008' #Acquired immune deficiency syndrome\n",
    "                   ] # Immunocompromised state\n",
    "\n",
    "rf2snomed['dementia'] = ['52448006'] #active\n",
    "\n",
    "#rf2snomed['heart_conditions'] = ['53741008', '84114007', '85898001'] \n",
    "rf2snomed['Coronary_arteriosclerosis'] =['53741008','443502000'] #active\n",
    "rf2snomed['Heart_failure'] =['84114007'] #active\n",
    "rf2snomed['Cardiomyopathy'] =['85898001'] #active\n",
    "\n",
    "print(len(rf2snomed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7531d47c-67ab-4c4f-85fa-9e5c6b140622",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_risk_factors(df, risk_factors_dict):\n",
    "  for risk_factor in risk_factors_dict:\n",
    "    codes = risk_factors_dict[risk_factor]\n",
    "    diagnosis_id_list = []\n",
    "    for code in codes:\n",
    "      df_dsi = get_dsi_from_snomed_descendant(code)\n",
    "      new_diag_id = [str(int(row.id)) for row in df_dsi.select('id').distinct().collect()]\n",
    "      diagnosis_id_list = diagnosis_id_list + new_diag_id\n",
    "    diagnosis_ids = \"','\".join(diagnosis_id_list)\n",
    "    \n",
    "    tmp =  spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT sm_positive_delta_results_feb.pat_id,sm_positive_delta_results_feb.instance, sm_positive_delta_results_feb.pat_enc_csn_id, IF(COUNT(diagnosis.dx_id) > 0, 'yes', 'no') AS \"\"\" + risk_factor + \"\"\" \n",
    "    FROM (((rdp_phi.problemlist\n",
    "      INNER JOIN rdp_phi.diagnosismapping ON rdp_phi.problemlist.dx_id = rdp_phi.diagnosismapping.dx_id)\n",
    "      INNER JOIN rdp_phi.diagnosis ON rdp_phi.diagnosismapping.dx_id = rdp_phi.diagnosis.dx_id)\n",
    "      INNER JOIN rdp_phi_sandbox.sm_positive_delta_results_feb ON problemlist.pat_id = sm_positive_delta_results_feb.pat_id AND problemlist.instance = sm_positive_delta_results_feb.instance)\n",
    "    WHERE problemlist.NOTED_DATE + INTERVAL 1 day < sm_positive_delta_results_feb.onehourbase \n",
    "      AND diagnosismapping.dx_id in ('\"\"\" + diagnosis_ids + \"\"\"')\n",
    "      AND (problemlist.problemstatus = \"Active\" OR problemlist.NOTED_DATE >= sm_positive_delta_results_feb.onehourbase - INTERVAL 730 day)\n",
    "    GROUP BY sm_positive_delta_results_feb.pat_id,sm_positive_delta_results_feb.instance, sm_positive_delta_results_feb.pat_enc_csn_id\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    print(risk_factor)\n",
    "    df = df.join(tmp, ['pat_id', 'pat_enc_csn_id','instance'], how='left').fillna({risk_factor: 'no'})\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de24702d-2687-4c3a-8adc-59a887a6835f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = add_risk_factors(df, rf2snomed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc4ec641-094d-42af-84bf-2d339d98eb89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf2snomed = {} # dictionary for mapping the risk factor names to their snomed codes\n",
    "\n",
    "rf2snomed['cancer'] = ['363346000'] # any kind of malignancy #active\n",
    "\n",
    "print(len(rf2snomed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e2c4197-ba41-4151-b82e-66afad9963c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_risk_factors(df, risk_factors_dict):\n",
    "  for risk_factor in risk_factors_dict:\n",
    "    codes = risk_factors_dict[risk_factor]\n",
    "    diagnosis_id_list = []\n",
    "    for code in codes:\n",
    "      df_dsi = get_dsi_from_snomed_descendant(code)\n",
    "      new_diag_id = [str(int(row.id)) for row in df_dsi.select('id').distinct().collect()]\n",
    "      diagnosis_id_list = diagnosis_id_list + new_diag_id\n",
    "    diagnosis_ids = \"','\".join(diagnosis_id_list)\n",
    "    \n",
    "    tmp =  spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT sm_positive_delta_results_feb.pat_id,sm_positive_delta_results_feb.instance, sm_positive_delta_results_feb.pat_enc_csn_id, IF(COUNT(diagnosis.dx_id) > 0, 'yes', 'no') AS \"\"\" + risk_factor + \"\"\" \n",
    "    FROM (((rdp_phi.problemlist\n",
    "      INNER JOIN rdp_phi.diagnosismapping ON rdp_phi.problemlist.dx_id = rdp_phi.diagnosismapping.dx_id)\n",
    "      INNER JOIN rdp_phi.diagnosis ON rdp_phi.diagnosismapping.dx_id = rdp_phi.diagnosis.dx_id)\n",
    "      INNER JOIN rdp_phi_sandbox.sm_positive_delta_results_feb ON problemlist.pat_id = sm_positive_delta_results_feb.pat_id AND problemlist.instance = sm_positive_delta_results_feb.instance)\n",
    "    WHERE problemlist.NOTED_DATE + INTERVAL 1 day < sm_positive_delta_results_feb.onehourbase \n",
    "      AND diagnosismapping.dx_id in ('\"\"\" + diagnosis_ids + \"\"\"')\n",
    "      AND (problemlist.problemstatus = \"Active\" OR problemlist.NOTED_DATE >= sm_positive_delta_results_feb.onehourbase - INTERVAL 1825 day)\n",
    "    GROUP BY sm_positive_delta_results_feb.pat_id,sm_positive_delta_results_feb.instance, sm_positive_delta_results_feb.pat_enc_csn_id\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    print(risk_factor)\n",
    "    df = df.join(tmp, ['pat_id', 'pat_enc_csn_id','instance'], how='left').fillna({risk_factor: 'no'})\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66482c48-7116-4e47-a650-ccee1fef65f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = add_risk_factors(df, rf2snomed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afa0058c-5590-472e-a9d6-e5ce0240548c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e95c6bb5-e9fe-4db2-9d67-355bd04f63af",
     "showTitle": true,
     "title": "CCI"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, lower\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "COMOR = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    problemlist.pat_id, \n",
    "    problemlist.instance,\n",
    "    CASE\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^b2[0-14]|b24' THEN 'AIDS'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^c(0[0-9]|1[0-9]|21|22|23|24|25|26|30|31|32|33|34|37|38|39|40|41|43|45|46|47|48|49|50|51|52|53|54|56|57|58|6[0-9]|70|71|72|73|74|75|76|81|82|83|84|85|88|90|91|92|93|94|95|96|97)' THEN 'Cancer'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^c7[7-9]|c80' THEN 'metastatic'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^g45|g46|h34|i6' THEN 'CD'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^i27[.]8|i27[.]9|j4[0-7]|j60|j61|j62|j63|j64|j65|j66|j67|j68[.]4|j70[.]1|j70[.]3' THEN 'CPD'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^i09[.]9|i11[.]0|i13[.]0|i13[.]2|i25[.]5|i42[.]0|i42[.]5|i42[.]6|i42[.]7|i42[.]8|i42[.]9|i43|i50|p29[.]0' THEN 'CHF'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^f0[0-3]|f05[.]1|g30|g31[.]1' THEN 'Dementia'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^e1[0-4][.]([01235679]|1)' THEN 'Diabetes'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^e1[0-4][.]([0345]|7)' THEN 'Diabetes_W'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^b18|k70[.]0|k70[.]1|k70[.]2|k70[.]3|k70[.]9|k71[.]3|k71[.]4|k71[.]5|k71[.]7|k73|k74|k76[.]0|k76[.]2|k76[.]3|k76[.]4|k76[.]8|k76[.]9|z94[.]4' THEN 'LD_M'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^i85[.]0|i85[.]9|i86[.]4|i98[.]2|k70[.]4|k71[.]1|k72[.]1|k72[.]9|k76[.]5|k76[.]6|k76[.]7' THEN 'LD_S'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^i21|i22|i25[.]2|i09[.]9|i11[.]0|i13[.]0|i13[.]2|i25[.]5|i42[.]0|i42[.]5|i42[.]6|i42[.]7|i42[.]8|i42[.]9|i43|i50|p29[.]0' THEN 'MI'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^k2[5-8]' THEN 'PUD'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^i70|i71|i73[.]1|i73[.]8|i73[.]9|i77[.]1|i79[.]0|i79[.]2|k55[.]1|k55[.]8|k55[.]9|z95[.]8|z95[.]9' THEN 'PVD'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^g04[.]1|g11[.]4|g80[.]1|g80[.]2|g81|g82|g83[.]0|g83[.]1|g83[.]2|g83[.]3|g83[.]4|g83[.]9' THEN 'Plegia'\n",
    "        WHEN lower(diagnosismapping.codes) RLIKE '^i12[.]0|i13[.]1|n03[.]2|n03[.]3|n03[.]4|n03[.]5|n03[.]6|n03[.]7|n05[.]2|n05[.]3|n05[.]4|n05[.]5|n05[.]6|n05[.]7|n18|n19|n25[.]0|z49[.]0|z49[.]1|z49[.]2' THEN 'RenalD'\n",
    "        ELSE 'Other'\n",
    "    END AS comorbidity\n",
    "FROM \n",
    "    problemlist\n",
    "JOIN \n",
    "    diagnosismapping \n",
    "ON \n",
    "    problemlist.code = diagnosismapping.code\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fb45f24-d461-4917-b8e7-cb3c60c7f9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "### This cell is to choose the highest rank among similar comorbidities such as cancer and metastatic\n",
    "w = Window.partitionBy(['pat_id','instance']).rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "\n",
    "COMOR1 = COMOR.withColumn(\"All_COMO\", (collect_list(col(\"Comorbidities\")).over(w)))\n",
    "\n",
    "COMOR1 = COMOR1.withColumn(\"Mutual\", \n",
    "                       when((array_contains(\"All_COMO\",\"Diabetes\")) & (array_contains(\"All_COMO\",\"Diabetes_W\")), \"Diabetes\")\n",
    "                       .when((array_contains(\"All_COMO\",\"Cancer\"))  & (array_contains(\"All_COMO\",\"metastatic\")), \"Cancer\")\n",
    "                       .when((array_contains(\"All_COMO\",\"LD_M\")) & (array_contains(\"All_COMO\",\"LD_S\")), \"LD_M\")\n",
    "                       .otherwise('ok'))\n",
    "\n",
    "COMOR1 = COMOR1.filter(COMOR1.Mutual != 'ok')\n",
    "\n",
    "COMOR1 = COMOR1.drop(COMOR1.All_COMO).drop(COMOR1.Comorbidities)\n",
    "\n",
    "COMOR1 = COMOR1.withColumnRenamed(\"Mutual\",\"Comorbidities\")\n",
    "\n",
    "COMOR = COMOR.join(COMOR1,[\"pat_id\",\"instance\",\"Comorbidities\"],'leftanti') #### ATTENTION IT IS LEFTANTI\n",
    "\n",
    "COMOR = COMOR.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e618ba3d-1c87-4e73-aa25-1ffab39da16d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "COMOR = COMOR.withColumn(\"scores\", \n",
    "                         when(COMOR.Comorbidities == \"AIDS\", 6)\n",
    "                        .when(COMOR.Comorbidities == \"Cancer\", 2)\n",
    "                        .when(COMOR.Comorbidities == \"metastatic\", 6)\n",
    "                        .when(COMOR.Comorbidities == \"CD\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"CPD\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"CHF\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"Dementia\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"Diabetes\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"Diabetes_W\", 2)\n",
    "                        .when(COMOR.Comorbidities == \"LD_M\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"LD_S\", 3)\n",
    "                        .when(COMOR.Comorbidities == \"MI\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"PUD\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"PVD\", 1)\n",
    "                        .when(COMOR.Comorbidities == \"Plegia\", 2)\n",
    "                        .when(COMOR.Comorbidities == \"RenalD\", 2)\n",
    "                        .when(COMOR.Comorbidities == \"Rheumatic\", 1))\n",
    "\n",
    "w = Window.partitionBy(['pat_id','instance']).rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "\n",
    "COMOR = COMOR.withColumn(\"CCI\", sum('scores').over(w))\n",
    "\n",
    "COMOR = COMOR.drop('Comorbidities').drop('scores')\n",
    "COMOR = COMOR.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "839997a5-023f-472c-8a4a-93708e75ca66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.join(COMOR,[\"pat_id\",\"instance\"],'left')\n",
    "df = df.withColumn('CCI',when(df.CCI.isNull(),0).otherwise(df.CCI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "125cbd6b-02cd-4642-a0d9-68be55f806e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Saving the updated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdde473e-b084-491d-a79b-4e28019185cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE rdp_phi_sandbox.sm_patt_delta_baseline_feb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c4aceaa-2feb-4077-a06d-79dda7e14cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = 'rdp_phi_sandbox.sm_patt_delta_baseline_feb'\n",
    "df.write.saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c9e114b-2f09-44ca-8550-bdd4c3d0ece2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "REFRESH table_name;\n",
    "SELECT * FROM rdp_phi_sandbox.sm_patt_delta_baseline_feb;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "PATT_Cohort_Baseline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
