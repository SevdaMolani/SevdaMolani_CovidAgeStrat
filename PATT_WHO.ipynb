{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fe6a2a6-5e87-4f3a-9bef-3bb9ef8cebad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run\n",
    "\"/Users/jennifer.hadlock2@providence.org/Jenn - Hadlock Lab Shared/Clinical Concepts/load_ceda_tools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9cd4a518-c923-4583-9731-9df0c9babbc4",
     "showTitle": true,
     "title": "Load CEDA API"
    }
   },
   "outputs": [],
   "source": [
    "%run\n",
    "\"/Users/jennifer.hadlock2@providence.org/Jenn - Hadlock Lab Shared/CEDA Tools v1.0/load_ceda_api\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7ed52eb-bafc-4588-b582-62ae541468db",
     "showTitle": true,
     "title": "Define utility functions and UDFs"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to floor datetime to nearest 6 hours\n",
    "def floor_datetime(dt):\n",
    "  if (dt is None):\n",
    "    return None\n",
    "  \n",
    "  # If string, convert to datetime\n",
    "  if (type(dt) == str):\n",
    "    dt_format = '%Y-%m-%d %H:%M:%S'\n",
    "    dt = datetime.datetime.strptime(dt, dt_format)\n",
    "  \n",
    "  # Floor datetime to nearest 6 hours\n",
    "  floored_hr = math.floor(dt.hour/6)*6\n",
    "  dt = dt.replace(hour=floored_hr, minute=0, second=0)\n",
    "  \n",
    "  return dt\n",
    "\n",
    "# Define UDF for floor_datetime\n",
    "floor_datetime_udf = F.udf(floor_datetime, TimestampType())\n",
    "\n",
    "# Utility function to get 6-hr datetime steps between two dates (inclusive)\n",
    "def get_timestamp_array(dt1, dt2):\n",
    "  # Empty array if there is no start datetime\n",
    "  if (dt1 is None):\n",
    "    return []\n",
    "  \n",
    "  # First datetime as single item if no stop datetime\n",
    "  if (dt2 is None):\n",
    "    return [floor_datetime(dt1)]\n",
    "  \n",
    "  # Convert and floor (to nearest 6 hours) both datetimes\n",
    "  dt1 = floor_datetime(dt1)\n",
    "  dt2 = floor_datetime(dt2)\n",
    "  \n",
    "  # Generate array of datetimes spaced by 6 hours\n",
    "  dt_array = []\n",
    "  while (dt1 <= dt2):\n",
    "    dt_array.append(dt1)\n",
    "    dt1 = dt1 + datetime.timedelta(hours=6)\n",
    "  \n",
    "  return dt_array\n",
    "\n",
    "# Define UDF for get_timestamp_array\n",
    "get_timestamp_array_udf = F.udf(get_timestamp_array, ArrayType(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5799bdda-2e42-4a8a-9d18-164441baf370",
     "showTitle": true,
     "title": "Specify date bounds"
    }
   },
   "outputs": [],
   "source": [
    "# Set date bounds for generating WHO scores\n",
    "date_bounds = ['2021-06-25', '2022-02-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f31e90-1c1a-4fe4-bafc-b94864f5b531",
     "showTitle": true,
     "title": "Get encounter records"
    }
   },
   "outputs": [],
   "source": [
    "# Get encounter records\n",
    "table_name = 'hadlock_encounters'\n",
    "column_selection = ['patient_id', 'encounter_id', 'contact_date', 'discharge_disposition']\n",
    "encounters_df = spark.sql(\"SELECT * FROM rdp_phi_sandbox.{}\".format(table_name)) \\\n",
    "  .where(F.col('contact_date').between(*date_bounds)) \\\n",
    "  .select(*column_selection, 'death_date', 'admission_datetime', 'discharge_datetime',\n",
    "          get_timestamp_array_udf(F.col('admission_datetime'), F.col('discharge_datetime')).alias('dt_array')) \\\n",
    "  .select(*column_selection,\n",
    "          floor_datetime_udf('death_date').alias('death_dt'),\n",
    "          floor_datetime_udf('admission_datetime').alias('admission_dt'),\n",
    "          floor_datetime_udf('discharge_datetime').alias('discharge_dt'),\n",
    "          F.explode('dt_array').alias('record_dt')) \\\n",
    "  .dropDuplicates()\n",
    "\n",
    "# Apply window functions\n",
    "partition_by_cols = ['patient_id', 'record_dt', 'death_dt']\n",
    "collect_set_cols = ['encounter_id', 'contact_date', 'admission_dt', 'discharge_dt', 'discharge_disposition']\n",
    "w = Window.partitionBy(partition_by_cols)\n",
    "encounters_agg_df = encounters_df \\\n",
    "  .select(*partition_by_cols,\n",
    "          *[F.collect_set(col).over(w).alias(col) for col in collect_set_cols]) \\\n",
    "  .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd958bc1-fff5-4790-bdc8-c2314ce2a798",
     "showTitle": true,
     "title": "Get device flowsheet records"
    }
   },
   "outputs": [],
   "source": [
    "# Get flowsheet measurement IDs for devices\n",
    "labels = ['oxygen_device', 'crrt', 'ecmo']\n",
    "flowsheet_measurement_ids = flowsheet_id_concept_map\n",
    "flowsheet_measurement_ids = {l: [str(k)+str(id_) for k, v in flowsheet_measurement_ids[l].items() for id_ in v] for l in labels}\n",
    "\n",
    "# Define UDF for adding T/F column identifying device records\n",
    "def is_device_record(flowsheet_meas_id, device):\n",
    "  if (flowsheet_meas_id is None):\n",
    "    return None\n",
    "  return str(flowsheet_meas_id) in flowsheet_measurement_ids[device]\n",
    "is_device_record_udf = F.udf(is_device_record, BooleanType())\n",
    "\n",
    "# Get flowsheet records\n",
    "table_name = 'hadlock_flowsheets'\n",
    "device_labels = ['oxygen_device', 'crrt', 'ecmo']\n",
    "column_selection = ['patient_id', 'flowsheet_measurement_id', 'recorded_datetime', 'value', 'name', 'encounter_id']\n",
    "flowsheets_df = spark.sql(\"SELECT * FROM rdp_phi_sandbox.{}\".format(table_name)) \\\n",
    "  .where(F.col('recorded_datetime').between(*date_bounds)) \\\n",
    "  .select(*column_selection, floor_datetime_udf(F.col('recorded_datetime')).alias('record_dt'),\n",
    "          *[is_device_record_udf(F.col('flowsheet_measurement_id'), F.lit(label)).alias(label) for label in device_labels]) \\\n",
    "  .where(' OR '.join(device_labels))\n",
    "\n",
    "# Apply window functions\n",
    "partition_by_cols = ['patient_id', 'record_dt']\n",
    "collect_set_cols = ['encounter_id', 'flowsheet_measurement_id', 'name', 'value']\n",
    "aliases = ['device_enc_id', 'flowsheet_meas_id', 'device_name', 'device_value']\n",
    "max_cols = device_labels\n",
    "w = Window.partitionBy(partition_by_cols)\n",
    "flowsheets_agg_df = flowsheets_df \\\n",
    "  .select(*partition_by_cols,\n",
    "          *[F.collect_set(c).over(w).alias(a) for c, a in zip(collect_set_cols, aliases)],\n",
    "          *[F.max(col).over(w).alias(col) for col in max_cols]) \\\n",
    "  .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be5b3d85-ec5f-49c1-95bb-46755d805a72",
     "showTitle": true,
     "title": "Get medication administration records"
    }
   },
   "outputs": [],
   "source": [
    "# Specify strings to search in order description\n",
    "# NOTE: Relying on RxNorm misses an appreciable number of records\n",
    "vasopressor_rxnorm = [\n",
    "  '3616',  # dobutamine\n",
    "  '3628',  # dopamine\n",
    "  '3992',  # epinephrine\n",
    "  '6963',  # midodrine\n",
    "  '7512',  # norepinephrine\n",
    "  '8163'   # phenylephrine\n",
    "]\n",
    "vasopressin_string = 'vasopressin'\n",
    "med_filter_string = \"(rxnorm IN ({}))\".format(', '.join([\"'{}'\".format(c) for c in vasopressor_rxnorm]))\n",
    "med_filter_string = med_filter_string + \" OR (lower(order_description) LIKE '%{}%')\".format(vasopressin_string)\n",
    "\n",
    "# Get RxNorm / medication ID mapping\n",
    "table_name = 'hadlock_rxnorm_medication_id_map'\n",
    "rxnorm_medication_id_map_df = spark.sql(\"SELECT * FROM rdp_phi_sandbox.{}\".format(table_name)) \\\n",
    "  .select('rxnorm', F.explode('medication_id').alias('medication_id')).dropDuplicates()\n",
    "\n",
    "# Get medication order/admin records\n",
    "table_name = 'hadlock_medication_orders'\n",
    "column_selection = ['patient_id', 'med_admin_or_order_dt', 'action_taken', 'order_description', 'encounter_id']\n",
    "med_admin_df = spark.sql(\"SELECT * FROM rdp_phi_sandbox.{}\".format(table_name)) \\\n",
    "  .withColumn('med_admin_or_order_dt',\n",
    "              F.when(F.col('administration_datetime').isNotNull(),\n",
    "                     F.col('administration_datetime')).otherwise(F.col('ordering_datetime'))) \\\n",
    "  .where(F.col('med_admin_or_order_dt').between(*date_bounds)) \\\n",
    "  .join(rxnorm_medication_id_map_df, ['medication_id'], how='left') \\\n",
    "  .where(med_filter_string) \\\n",
    "  .select(*column_selection, 'rxnorm', 'medication_id',\n",
    "          floor_datetime_udf(F.col('med_admin_or_order_dt')).alias('record_dt'))\n",
    "\n",
    "# Apply window functions\n",
    "partition_by_cols = ['patient_id', 'record_dt']\n",
    "collect_set_cols = ['encounter_id', 'rxnorm', 'medication_id', 'order_description', 'action_taken']\n",
    "aliases = ['med_order_enc_id', 'rxnorm_codes', 'medication_id', 'med_order_description', 'action_taken']\n",
    "w = Window.partitionBy(partition_by_cols)\n",
    "med_admin_agg_df = med_admin_df \\\n",
    "  .select(*partition_by_cols,\n",
    "          *[F.collect_set(c).over(w).alias(a) for c, a in zip(collect_set_cols, aliases)]) \\\n",
    "  .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6458dd38-f9e5-4e56-b4f4-1d412c7bbbef",
     "showTitle": true,
     "title": "Confirm there are no column name conflicts"
    }
   },
   "outputs": [],
   "source": [
    "encounter_cols = set(encounters_agg_df.columns)\n",
    "flowsheet_cols = set(flowsheets_agg_df.columns)\n",
    "med_admin_cols = set(med_admin_agg_df.columns)\n",
    "assert(encounter_cols.intersection(flowsheet_cols) == {'record_dt', 'patient_id'})\n",
    "assert(encounter_cols.intersection(flowsheet_cols) == encounter_cols.intersection(med_admin_cols))\n",
    "assert(encounter_cols.intersection(med_admin_cols) == flowsheet_cols.intersection(med_admin_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7ec329-d507-4ca4-9e14-70b435bea032",
     "showTitle": true,
     "title": "Define function to calculate WHO score"
    }
   },
   "outputs": [],
   "source": [
    "def get_who_score(record_dt, death_dt, admit_dt, disch_dt, disch_disp, oxygen, crrt, ecmo, device_value, med_action_taken):\n",
    "  # Specify oxygen device strings\n",
    "  device_string_map = {\n",
    "    6: ['mechanical ventilation', 'manual bag ventilation', 'ventilator'],\n",
    "    5: ['high-flow nasal cannula', 'hfnc', 'non-invasive ventilation', 'cpap', 'bi-pap', 'bipap', 'high frequency ventilation', 'et tube', 'ett', 't-piece'],\n",
    "    4: ['nasal cannula', 'simple face mask', 'open oxygen mask', 'tracheostomy collar', 'nonrebreather mask', 'non-rebreather mask', 'nrb mask', 'cannula', 'mask-simple', 'o2/cannula', 'o2/simple mask', 'simple mask', 'mask-nrb', 'mask-aerosol', 'continuous aerosol', 'oxymask', 'o2 via face mask', 'venti-mask', 'vapotherm', 'laryngeal mask airway', 'aerosol mask', 'vapotherm', 'face tent', 'oxygen tent'],\n",
    "    3: ['room air']\n",
    "  }\n",
    "  \n",
    "  # Check if patient is expired\n",
    "  if (not(death_dt is None)):\n",
    "    if (record_dt >= death_dt):\n",
    "      return 8\n",
    "  elif (not(disch_dt is None) and (len(disch_dt) > 0)):\n",
    "    if (not(disch_disp is None) and ('Expired' in disch_disp)):\n",
    "      if (record_dt >= max(disch_dt)):\n",
    "        return 8\n",
    "  \n",
    "  # Missing oxygen record\n",
    "  if ((oxygen is None) or (oxygen == False)):\n",
    "    # Implement logic for assignments based on admit and discharge datetimes\n",
    "    if (not(admit_dt is None) and (len(admit_dt) > 0)):\n",
    "      if (not(disch_dt is None) and (len(disch_dt) > 0)):\n",
    "        # 3 if record datetime is on or after admit and before discharge datetime\n",
    "        if ((record_dt >= min(admit_dt)) and (record_dt < max(disch_dt))):\n",
    "          return 3\n",
    "        # 2 if on discharge datetime\n",
    "        elif (record_dt == max(disch_dt)):\n",
    "          return 2\n",
    "      else:\n",
    "        # 3 if record datetime coincides with admit datetime\n",
    "        if (record_dt == min(admit_dt)):\n",
    "          return 3\n",
    "    else:\n",
    "      # Cannot determine score without admit and discharge datetimes\n",
    "      return None\n",
    "  \n",
    "  # Get nominal WHO score based on oxygen device\n",
    "  who_score = None\n",
    "  if (not(device_value is None)):\n",
    "    oxygen_device_vals = ';'.join([s.lower() for s in device_value])\n",
    "    for i in [6, 5, 4, 3]:\n",
    "      if (any([(s in oxygen_device_vals) for s in device_string_map[i]])):\n",
    "        who_score = i\n",
    "        break\n",
    "    # If 3 and record datetime coincides with discharge, set to 2\n",
    "    if (who_score == 3):\n",
    "      if (not(disch_dt is None) and (len(disch_dt) > 0)):\n",
    "        if (record_dt == max(disch_dt)):\n",
    "          who_score = 2\n",
    "  \n",
    "  # If 6 and additional organ support, adjust up to 7\n",
    "  if (who_score == 6):\n",
    "    # CRRT or ECMO\n",
    "    if ((crrt == True) or (ecmo == True)):\n",
    "      who_score = 7\n",
    "    # Vasopressor\n",
    "    elif (not(med_action_taken is None) and (len(med_action_taken) > 0)):\n",
    "      actions = ['Rate Change', 'New Bag', 'Given', 'Restarted', 'Rate Verify']\n",
    "      if (any([(s in med_action_taken) for s in actions])):\n",
    "        who_score = 7\n",
    "  \n",
    "  return who_score\n",
    "\n",
    "# Create UDF\n",
    "get_who_score_udf = F.udf(get_who_score, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09ab91bc-9844-40b2-a613-d411c5832f87",
     "showTitle": true,
     "title": "Join dataframes and compute WHO scores"
    }
   },
   "outputs": [],
   "source": [
    "# Get full set of patients and record datetimes\n",
    "join_cols = ['patient_id', 'record_dt']\n",
    "select_cols_1 = ['death_dt', 'encounter_id', 'contact_date', 'admission_dt', 'discharge_dt', 'discharge_disposition']\n",
    "who_arg_cols = ['record_dt', 'death_dt', 'admission_dt', 'discharge_dt', 'discharge_disposition', 'oxygen_device', 'crrt', 'ecmo', 'device_value', 'action_taken']\n",
    "select_cols_2 = ['oxygen_device', 'crrt', 'ecmo', 'device_enc_id', 'device_name', 'device_value', 'med_order_enc_id', 'rxnorm_codes', 'med_order_description', 'action_taken']\n",
    "\n",
    "who_scores_df = encounters_agg_df \\\n",
    "  .join(flowsheets_agg_df, join_cols, how='full') \\\n",
    "  .join(med_admin_agg_df, join_cols, how='full') \\\n",
    "  .select(*join_cols, *select_cols_1,\n",
    "          get_who_score_udf(*[F.col(c) for c in who_arg_cols]).alias('who_score'),\n",
    "          *select_cols_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f89b8f-e511-44d4-b581-c1e918366c30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE rdp_phi_sandbox.sm_who_scores_feb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "611b4bf4-f3c1-404a-9a2a-6e9cf46d4f5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "who_scores_df.write.saveAsTable(\"rdp_phi_sandbox.sm_who_scores_feb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61838247-6804-40e0-b459-06e22c0cbc43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "PATT_WHO",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
